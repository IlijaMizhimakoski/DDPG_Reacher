{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f901076-8881-400e-8281-92e56d4de08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'ddpg_reacher_simple' from '/app/ddpg_reacher_simple.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import ddpg_reacher_simple as d\n",
    "import importlib\n",
    "import cv2 \n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "importlib.reload(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519321e6-6502-4907-83e1-ebafaa2faedb",
   "metadata": {},
   "source": [
    "#### Function for real time reward plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4faee93c-d484-4dbf-9470-b4289fbfd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards_threaded(interval=1.0, smooth_window=10):\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_scatter(x=[], y=[], mode='lines+markers', name='Smoothed Reward')\n",
    "    fig.update_layout(\n",
    "        title='Episode Rewards Over Time',\n",
    "        xaxis_title='Episode',\n",
    "        yaxis_title='Reward',\n",
    "        yaxis_range=[0, None]\n",
    "    )\n",
    "    display(fig)\n",
    "\n",
    "    def plot_loop():\n",
    "        while True:\n",
    "            time.sleep(interval)\n",
    "            if episode_rewards:\n",
    "                smoothed = [\n",
    "                    sum(episode_rewards[max(0, i - smooth_window):i + 1]) / (i - max(0, i - smooth_window) + 1)\n",
    "                    for i in range(len(episode_rewards))\n",
    "                ]\n",
    "                with fig.batch_update():\n",
    "                    fig.data[0].x = list(range(len(smoothed)))\n",
    "                    fig.data[0].y = smoothed\n",
    "\n",
    "    t = threading.Thread(target=plot_loop, daemon=True)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd211ce-6cf7-47b1-bf32-943abeefc891",
   "metadata": {},
   "source": [
    "#### Initialize  agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35e5a68a-6e6b-4279-81e9-740af92ad71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v5\",reward_control_weight=0.15, render_mode=None)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "buffer = d.ReplayBuffer(capacity=1_000_000, state_dim=state_dim)\n",
    "agent = d.DDPGAgent(state_dim, action_dim, buffer)\n",
    "num_episodes = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ebc9f-f5c2-4dc7-8ab8-e0bd5df4ec91",
   "metadata": {},
   "source": [
    "#### Train agent and plot the episode accumulated reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ea00909-04f0-451a-a7c7-ffb333b8af16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3a03ad319a47be9d26f64e099ac13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Smoothed Reward',\n",
       "              'type': 'scatter',\n",
       "              'uid': '2475236d-bd4f-414c-94fa-9726204a779d',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Episode Rewards Over Time'},\n",
       "               'xaxis': {'title': {'text': 'Episode'}},\n",
       "               'yaxis': {'range': [0, None], 'title': {'text': 'Reward'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "plot_rewards_threaded()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    agent.ou_noise.reset()\n",
    "    cumm_reward = 0\n",
    "    terminated = truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        action = agent.choose_actions(state).cpu().numpy().flatten()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        agent.train()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        cumm_reward += reward\n",
    "    \n",
    "    episode_rewards.append(cumm_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336569ee-cdf4-49bb-8ff5-ac048e5fed20",
   "metadata": {},
   "source": [
    "#### Save the actor and critic models and test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "553f5d59-6219-4ed9-b191-1eddf4523480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full actor model saved to actor_best.pth\n",
      "Full critic model saved to critic_best.pth\n"
     ]
    }
   ],
   "source": [
    "agent.save_full_model(\"actor_best.pth\",\"critic_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d6c1ae2-36e1-49b5-94fc-5bb1063076b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Reacher-v5\", render_mode=\"human\")\n",
    "test_episodes = 50\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    terminated = truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        action = agent.policy(state).cpu().numpy().flatten()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a246e-5ab1-49d1-88de-5a625370d77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
