{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "307e5b39-7f4c-480f-8045-4e85cd121031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'ddpg_reacher_simple' from '/app/ddpg_reacher_simple.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import ddpg_reacher_simple as d\n",
    "import importlib\n",
    "import cv2 \n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "importlib.reload(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470408f1-2148-45a8-b075-9d9ee7752421",
   "metadata": {},
   "source": [
    "#### Function for evaluating the agent based on reward metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70da6e92-31c5-46bc-baa6-0e6b32829a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env_name, n_episodes=50, cumulative_reward_threshold=-3):\n",
    "    \"\"\"\n",
    "    Evaluates a DDPG agent over a specified number of episodes using two metrics:\n",
    "    1. Accumulated reward average.\n",
    "    2. Success rate defined by staying at the target for a minimum number of timesteps.\n",
    "\n",
    "    Args:\n",
    "        agent (DDPGAgent): The trained DDPG agent object with a .policy() method.\n",
    "        env_name (str): The name of the Gymnasium environment (e.g., 'Reacher-v4').\n",
    "        n_episodes (int): The number of episodes to run for the test.\n",
    "        success_distance_threshold (float): Max L2 distance between fingertip and target to count as \"at target.\"\n",
    "        success_time_steps (int): Minimum consecutive timesteps required to count a successful episode.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the average reward and success rate\n",
    "        reward_control_weight=0.1,.\n",
    "    \"\"\"\n",
    "    eval_env = gym.make(env_name, reward_control_weight=0.1, render_mode='human')\n",
    "    total_rewards = []\n",
    "    successful_episodes = 0\n",
    "\n",
    "    best_reward = -np.inf\n",
    "    best_episode_index = -1\n",
    "\n",
    "    print(f\"Starting evaluation of {n_episodes} episodes...\")\n",
    "    print(f\"Using success definition: Cumulative Reward > {cumulative_reward_threshold}\")\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, info = eval_env.reset()\n",
    "        episode_reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action_tensor = agent.policy(state)\n",
    "           \n",
    "            action = action_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "            state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "        if episode_reward > cumulative_reward_threshold:\n",
    "            successful_episodes += 1\n",
    "\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_episode_index = episode\n",
    "\n",
    "    eval_env.close()\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    success_rate = successful_episodes / n_episodes\n",
    "\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"Accumulated Reward Average: **{avg_reward:.4f}**\")\n",
    "    print(f\"Success Rate (Cumulative Reward > {cumulative_reward_threshold}): **{success_rate*100:.2f}%** ({successful_episodes}/{n_episodes})\")\n",
    "    print(f\"Best Episode Reward: {best_reward:.4f} (Episode: {best_episode_index})\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        'avg_reward_50_episodes': avg_reward,\n",
    "        'success_rate_cumulative_reward': success_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492ea4c-ba7b-4fff-815b-6216b8eaeb4b",
   "metadata": {},
   "source": [
    "#### Load the actor and critic models and initialize agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba2b7099-728b-469f-aa6d-ba1b66ea48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full critic model loaded from critic_best.pth\n",
      "Full actor model loaded from actor_best.pth\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Reacher-v5\",reward_control_weight=0.18, render_mode=None)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "buffer = d.ReplayBuffer(capacity=1_000_000, state_dim=state_dim)\n",
    "agent = d.DDPGAgent(state_dim, action_dim, buffer)\n",
    "agent.load_full_model('actor_best.pth','critic_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa7bbc-e69b-407d-8ac2-ee7aab17b5aa",
   "metadata": {},
   "source": [
    "#### Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb8ee711-f054-42e1-a145-b692a283a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of 500 episodes...\n",
      "Using success definition: Cumulative Reward > -3\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "Accumulated Reward Average: **-2.7973**\n",
      "Success Rate (Cumulative Reward > -3): **68.00%** (340/500)\n",
      "Best Episode Reward: -0.8485 (Episode: 179)\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate_agent(\n",
    "    agent=agent, \n",
    "    env_name='Reacher-v5', \n",
    "    n_episodes=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d18d8a-2bed-4a6a-9d46-6dfc58191c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
